{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import analytiq_data as ad\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.common.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MONGODB_URI environment variable\n",
    "os.environ[\"MONGODB_URI\"] = \"mongodb://localhost:27017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qP8xDKSfu6Ede7pbEmUWc6I6ii7sOAHr\n"
     ]
    }
   ],
   "source": [
    "analytiq_client = ad.common.get_analytiq_client(env=\"test\")\n",
    "\n",
    "llm_key = await ad.llm.get_llm_key(analytiq_client, llm_provider=\"mistral\")\n",
    "print(llm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('67fb329275436ea808f0c0b5', 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_id = \"6795345439604beca2b2808d\"\n",
    "prompt_id, version = await ad.common.get_prompt_id_and_version(analytiq_client, \"cv\", org_id)\n",
    "prompt_id, version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6841c24a7cf0dd681b6d6124'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_rev_id = await ad.common.get_prompt_rev_id(analytiq_client, prompt_id, version)\n",
    "prompt_rev_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rev_id = \"6840ae34ca71c1ea380f7dd0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = \"683ffcfce640fb757220edfe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'claude-3-7-sonnet-latest'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await ad.llm.get_llm_model(analytiq_client, prompt_rev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "#litellm._turn_on_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 12:26:33,859 - analytiq_data.llm.llm - INFO - Running new LLM analysis for document_id: 683ffcfce640fb757220edfe, prompt_rev_id: 6840ae34ca71c1ea380f7dd0\n",
      "2025-06-05 12:26:33,860 - analytiq_data.llm.llm - INFO - LLM model: azure_ai/deepseek-v3, provider: azure_ai, api_key: ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 12:26:33,863 - analytiq_data.llm.llm - INFO - No response format found for prompt 6840ae34ca71c1ea380f7dd0\n",
      "\u001b[92m12:26:33 - LiteLLM:INFO\u001b[0m: utils.py:2826 - \n",
      "LiteLLM completion() model= deepseek-v3; provider = azure_ai\n",
      "2025-06-05 12:26:33,864 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= deepseek-v3; provider = azure_ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "litellm.APIConnectionError: APIConnectionError: Azure_aiException - api_base is required for Azure AI Studio. Please set the api_base parameter. Passed `api_base=None`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:1528\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m     logging.post_call(\n\u001b[32m   1523\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1524\u001b[39m         api_key=api_key,\n\u001b[32m   1525\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1526\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1527\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1531\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:1503\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1502\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1503\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   1516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1521\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:261\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config)\u001b[39m\n\u001b[32m    251\u001b[39m headers = provider_config.validate_environment(\n\u001b[32m    252\u001b[39m     api_key=api_key,\n\u001b[32m    253\u001b[39m     headers=headers \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m     litellm_params=litellm_params,\n\u001b[32m    259\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m api_base = \u001b[43mprovider_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_complete_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m data = provider_config.transform_request(\n\u001b[32m    271\u001b[39m     model=model,\n\u001b[32m    272\u001b[39m     messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     headers=headers,\n\u001b[32m    276\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/azure_ai/chat/transformation.py:97\u001b[39m, in \u001b[36mAzureAIStudioConfig.get_complete_url\u001b[39m\u001b[34m(self, api_base, api_key, model, optional_params, litellm_params, stream)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mapi_base is required for Azure AI Studio. Please set the api_base parameter. Passed `api_base=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m     )\n\u001b[32m    100\u001b[39m original_url = httpx.URL(api_base)\n",
      "\u001b[31mValueError\u001b[39m: api_base is required for Azure AI Studio. Please set the api_base parameter. Passed `api_base=None`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm_result = \u001b[38;5;28;01mawait\u001b[39;00m ad.llm.run_llm(analytiq_client,\n\u001b[32m      2\u001b[39m                                   document_id=document_id,\n\u001b[32m      3\u001b[39m                                   prompt_rev_id=prompt_rev_id,\n\u001b[32m      4\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"groq/deepseek-r1-distill-llama-70b\",\u001b[39;00m\n\u001b[32m      5\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gemini/gemini-2.0-flash\",\u001b[39;00m\n\u001b[32m      6\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gemini/gemini-2.5-flash-preview-05-20\",\u001b[39;00m\n\u001b[32m      7\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gemini/gemini-2.5-flash-preview-tts\",\u001b[39;00m\n\u001b[32m      8\u001b[39m                                   llm_model=\u001b[33m\"\u001b[39m\u001b[33mazure_ai/deepseek-v3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gpt-4o-mini\",\u001b[39;00m\n\u001b[32m     10\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gpt-4o-2024-08-06\",\u001b[39;00m\n\u001b[32m     11\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"claude-3-5-sonnet\",\u001b[39;00m\n\u001b[32m     12\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"mistral/mistral-large-latest\",\u001b[39;00m\n\u001b[32m     13\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"mistral/open-mixtral-8x22b\",\u001b[39;00m\n\u001b[32m     14\u001b[39m                                   force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m llm_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/packages/analytiq_data/notebooks/tests/../../../analytiq_data/llm/llm.py:88\u001b[39m, in \u001b[36mrun_llm\u001b[39m\u001b[34m(analytiq_client, document_id, prompt_rev_id, llm_model, force)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     86\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo response format found for prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_rev_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m litellm.acompletion(\n\u001b[32m     89\u001b[39m     model=llm_model,\n\u001b[32m     90\u001b[39m     messages=[\n\u001b[32m     91\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m     92\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}\n\u001b[32m     93\u001b[39m     ],\n\u001b[32m     94\u001b[39m     api_key=api_key,\n\u001b[32m     95\u001b[39m     temperature=\u001b[32m0.1\u001b[39m,\n\u001b[32m     96\u001b[39m     response_format=response_format\n\u001b[32m     97\u001b[39m )\n\u001b[32m     99\u001b[39m resp_dict = json.loads(response.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# If this is not the default prompt, reorder the response to match schema\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/utils.py:1460\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1458\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1459\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/utils.py:1321\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _caching_handler_response.final_embedding_cached_response\n\u001b[32m   1320\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1322\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:523\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    522\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m    524\u001b[39m         model=model,\n\u001b[32m    525\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m    526\u001b[39m         original_exception=e,\n\u001b[32m    527\u001b[39m         completion_kwargs=completion_kwargs,\n\u001b[32m    528\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m    529\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:496\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m ctx = contextvars.copy_context()\n\u001b[32m    494\u001b[39m func_with_context = partial(ctx.run, func)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m init_response = \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_with_context)\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_response, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    498\u001b[39m     init_response, ModelResponse\n\u001b[32m    499\u001b[39m ):  \u001b[38;5;66;03m## CACHING SCENARIO\u001b[39;00m\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_response, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/utils.py:989\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    986\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMax retries per request hit!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    988\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcomplete_response\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[32m    993\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mcomplete_response\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    994\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:3213\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3212\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3216\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2229\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, error_type):\n\u001b[32m   2228\u001b[39m         \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e  \u001b[38;5;66;03m# it's already mapped\u001b[39;00m\n\u001b[32m   2230\u001b[39m raised_exc = APIConnectionError(\n\u001b[32m   2231\u001b[39m     message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(original_exception, traceback.format_exc()),\n\u001b[32m   2232\u001b[39m     llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2233\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2234\u001b[39m )\n\u001b[32m   2235\u001b[39m \u001b[38;5;28msetattr\u001b[39m(raised_exc, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:470\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m    460\u001b[39m                 status_code=original_exception.status_code,\n\u001b[32m    461\u001b[39m                 message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPIError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    465\u001b[39m                 litellm_debug_info=extra_information,\n\u001b[32m    466\u001b[39m             )\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;66;03m# if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors\u001b[39;00m\n\u001b[32m    469\u001b[39m         \u001b[38;5;66;03m# exception_mapping_worked = True\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m    471\u001b[39m             message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPIConnectionError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    472\u001b[39m             llm_provider=custom_llm_provider,\n\u001b[32m    473\u001b[39m             model=model,\n\u001b[32m    474\u001b[39m             litellm_debug_info=extra_information,\n\u001b[32m    475\u001b[39m             request=httpx.Request(\n\u001b[32m    476\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m             ),\n\u001b[32m    478\u001b[39m         )\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    480\u001b[39m     custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33manthropic\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    481\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33manthropic_text\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m ):  \u001b[38;5;66;03m# one of the anthropics\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt is too long\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt: length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str:\n",
      "\u001b[31mAPIConnectionError\u001b[39m: litellm.APIConnectionError: APIConnectionError: Azure_aiException - api_base is required for Azure AI Studio. Please set the api_base parameter. Passed `api_base=None`"
     ]
    }
   ],
   "source": [
    "llm_result = await ad.llm.run_llm(analytiq_client,\n",
    "                                  document_id=document_id,\n",
    "                                  prompt_rev_id=prompt_rev_id,\n",
    "                                  #llm_model=\"groq/deepseek-r1-distill-llama-70b\",\n",
    "                                  #llm_model=\"gemini/gemini-2.0-flash\",\n",
    "                                  #llm_model=\"gemini/gemini-2.5-flash-preview-05-20\",\n",
    "                                  #llm_model=\"gemini/gemini-2.5-flash-preview-tts\",\n",
    "                                  llm_model=\"azure_ai/deepseek-v3\",\n",
    "                                  #llm_model=\"gpt-4o-mini\",\n",
    "                                  #llm_model=\"gpt-4o-2024-08-06\",\n",
    "                                  #llm_model=\"claude-3-5-sonnet\",\n",
    "                                  #llm_model=\"mistral/mistral-large-latest\",\n",
    "                                  #llm_model=\"mistral/open-mixtral-8x22b\",\n",
    "                                  force=True)\n",
    "llm_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result_id = await ad.llm.save_llm_result(analytiq_client,\n",
    "                                             document_id=document_id,\n",
    "                                             prompt_rev_id=prompt_rev_id,\n",
    "                                             llm_result=llm_result)\n",
    "llm_result_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = await ad.llm.get_llm_result(analytiq_client,\n",
    "                                         document_id=document_id,\n",
    "                                         prompt_rev_id=prompt_rev_id)\n",
    "llm_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.delete_llm_result(analytiq_client,\n",
    "                                document_id=document_id,\n",
    "                                prompt_rev_id=prompt_rev_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.run_llm_for_prompt_rev_ids(analytiq_client,\n",
    "                                        document_id=document_id,\n",
    "                                        prompt_rev_ids=[prompt_rev_id],\n",
    "                                        model=\"groq/deepseek-r1-distill-llama-70b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm.utils import supports_response_schema\n",
    "\n",
    "supports_response_schema(model=\"groq/deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.list_llm_providers(analytiq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.llm.is_chat_model(\"gemini/gemini-2.5-flash-preview-tts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.providers.setup_llm_providers(analytiq_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
